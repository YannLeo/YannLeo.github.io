# 幻觉的评估

## 三、如何评估大模型幻觉问题？

现有的传统幻觉评估指标和人类结果的相关性往往较低，同时大都是task-specific（基于特定任务）的。

关于大模型幻觉评估，可以分为**基于参考的评估**和**无参考的评估**。

### 3.1 Reference-based（基于参考的评估）

Reference-based方法是指在评估生成内容的准确性时，使用**参考文本**（如人类生成的标准答案）或**原始信息源**作为对比基准。

**这种方法主要衡量模型生成内容和参考答案的重要程度或相似度**，通常使用的指标包括ROUGE、BLEU等。

- **优点**：可以通过参考标准的答案来评估生成内容的相似性，**适合一些标准化的生成任务**。
- **缺点**：在许多任务中，参考答案可能并非唯一；生成内容有多种表达方式，因此**该方法的评估灵活性有限**。此外，对于大语言模型生成的开放式内容，不一定能找到完美的参考答案。

Reference-based的指标有两类：

- 基于Source Information和Target Reference：利用一些统计学指标，比如ROUGE、BLEU来评估输出结果和Source/Target信息的重叠度；
- 基于Source Information：由于NLG任务里，Target输出往往是多种多样的，因此许多工作只基于Source信息进行幻觉的评估。比如Knowledge F1。

> **Knowledge F1** 是一种用于评估自然语言生成（NLG）任务的指标，它主要用于检测模型输出中的幻觉现象。

> 它的基本思路是**比较模型生成的内容和参考的知识来源之间的匹配度，来判断生成内容的准确性和一致性**。

> Knowledge F1 的计算原理

> 1. **知识检索**：首先，从任务相关的知识库或上下文信息中提取模型生成时可参考的**源知识**。
> 2. **知识匹配**：然后，将模型的生成输出和源知识中的信息进行比对，找出生成文本中哪些部分是与源知识一致的。
> 3. **F1得分计算**：
>     - 最后，通过精确率（Precision）和召回率（Recall）来计算F1得分：
>          - **精确率**：生成内容中与源知识匹配的信息占生成内容总信息的比例。
>          - **召回率**：生成内容中与源知识匹配的信息占源知识总信息的比例。

>  为什么 Knowledge F1 适合评估幻觉

> - Knowledge F1 只依赖源知识，不需要一个单一的参考输出。因此，它能在内容多样化的生成任务中有效评估模型的幻觉问题，避免单一答案的限制。高 F1 得分意味着模型生成的内容和源知识的一致性高，幻觉可能性较低。低得分可能表明模型生成了不准确的信息，即发生了"幻觉"。

基于Reference的评价指标，基本上只能评价Faithfulness，而无法评价Factualness，因此通常不适用于LLMs。


### 3.2 Reference-Free（无参考评估）

在大语言模型（LLM）生成内容的幻觉问题中，Reference-Free（无参考评估）方法**旨在不用标准答案或特定参考来检测模型生成内容的准确性和一致性**。

这里主要介绍了几种方法，包括基于**信息抽取（IE）、基于问答（QA）、基于自然语言推理（NLI）、基于事实性分类，以及人工评估**。这些方法的核心理念和优缺点如下：

#### 3.2.1 基于IE（信息抽取）

**方法介绍：**

- 基于信息抽取（IE）的评估方法会**将生成内容转化为结构化的知识**，例如三元组（subject-predicate-object的关系形式），以便进一步验证其真实性。
- 具体来说，这种方法**先用一个IE模型从生成内容中提取出三元组表示的关系和事件，然后通过另一个验证模型对这些信息是否准确**。

**缺点：**

- **错误传播：** IE模型本身可能会出错，如果IE模型提取的信息有误，后续的验证也会失去准确性。
- **信息受限：** 这种方法仅限于可以转化为三元组的知识，而很多复杂的信息难以用简单三元组表示，导致评估局限性。

#### 3.2.2 基于QA（问答）

**方法介绍：**

- 基于问答（QA）的评估方法通过生成问题-答案来检测内容准确性。具体流程为：
     1. 首先，**使用一个问题生成（QG, question generation）模型，根据模型的生成内容产生一系列相关的问题-答案对（QA pairs）**。
     2. 然后，**利用源信息，使用问答模型回答这些问题**。
     3. 最后，**将问答模型的答案与最初生成的答案对比，通过匹配度评估生成内容的真实性**。

**缺点：**

- **IE模型的错误传播问题：** QA过程依赖IE模型生成的问题，若IE模型出错会影响QA结果。
- **难以评估Factualness：** QA模型回答问题时，源信息不一定包含所有所需知识，可能出现无法准确回答或验证的情况，从而影响评估质量。


#### 3.2.3 基于NLI（自然语言推理）

**方法介绍：**

- 基于自然语言推理（NLI）的评估方法通过验证生成文本是否由源信息"**蕴含**"来判断其是否具有幻觉。
- 此方法**使用NLI模型检查生成内容是否与源信息存在"蕴含"关系，以此作为模型生成内容真实性的标志。**

**缺点：**

- **性能有限：** 现成的NLI模型在事实核查方面表现一般，难以准确验证生成内容。
- **无法检测世界知识相关幻觉：** NLI模型只能基于源信息进行蕴含验证，难以处理需要世界知识的幻觉检测。
- **粒度限制：** NLI通常在句子级别进行核查，无法进行更细粒度的分析。
- **幻觉和蕴含不等价：** 幻觉不仅仅是"不蕴含"，比如"Putin is president"和"Putin is U.S. president"在语义上并非幻觉，但会被判断为蕴含。

    > 在NLI中，"蕴含"意味着一个句子能够逻辑上推导出另一个句子，但它并不涉及判断信息的真实与否。比如上面这个例子：

    >   - "Putin is president"这句话意味着普京确实担任某个国家的总统。
    >   - "Putin is U.S. president"则暗示普京是美国的总统。

    > 在NLI的判别规则下，后者可以被理解为前者的"蕴含"，因为"Putin is president"所表达的信息确实可以推导出"某国的总统"这一概念，但在现实中这是错误的，属于幻觉（因为普京并不是美国总统）。

    > 也就是说，NLI的"蕴含"仅仅是在逻辑上匹配，而不关心语义上是否真实一致。因此，仅用NLI模型来判断"蕴含"关系可能会错判某些幻觉内容。


#### 3.2.4 基于Factualness Classification Metric（事实性分类指标）

**方法介绍：**

- 这一方法**通过人工标注或构造包含幻觉和真实信息的数据集，训练分类模型检测新生成文本是否符合事实，进而判断是否发生了幻觉**。

**缺点：**

- 依赖大量标注数据集训练，这一过程成本较高，且分类模型在检测复杂幻觉时可能有局限。

#### 3.2.5 人工评估

**方法介绍：**

- 人工评估是最准确的方式，尤其适用于检测复杂的幻觉现象。

**优缺点：**

- **优点：** 最为可靠，尤其在人类评估无法全面覆盖的情况下。
- **缺点：** 人工评估成本较高。

---

### 3.3 总结

关于大模型幻觉评估，可以分为基于参考的评估和无参考的评估。

- Reference-based（基于参考的评估）：  
  使用**参考文本**（如人类生成的标准答案）或**原始信息源**作为对比基准。这种方法**主要衡量模型生成内容和参考答案的重叠程度或相似度**，通常使用的指标包括ROUGE、BLEU等。
  
- Reference-Free（无参考评估）：  
  **该方法旨在不用标准答案或特定参考来检测模型生成内容的准确性和一致性**。此处介绍了几种方法，包括基于信息抽取（IE）、基于问答（QA）、基于自然语言推理（NLI）、基于事实性分类，以及人工评估。