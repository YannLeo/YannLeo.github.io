# LLM训练任务/目标

## 6. LLMs 中，常用的预训练任务包含哪些？（LLMs 的训练目标是什么？）

在进行模型的大规模预训练时，往往需要设计合适的自监督预训练任务，使得模型能够从海量无标签数据中学习到广泛的语义知识与世界知识。

目前，常用的预训练任务主要分为三类，包括**语言建模（Language Modeling, LM）**、**去噪自编码（Denoising Autoencoding, DAE）**以及**混合去噪器（Mixture-of-Denoisers, MoD）**。

> 下图展示了语言建模和去噪自编码各自的输入与输出示例。

<center>
    <img src="../fig/04-tasks.png">
    <center>语言建模与去噪自编码的输入输出对比</center>
</center>


### 语言建模（Language Modeling, LM）

语言建模任务是目前绝大部分大语言模型广泛采用的预训练任务。该任务的核心在于**“预测下一个词元”**，并且经常被应用于训练基于解码器的大语言模型，例如 GPT-3 和 PaLM 等。

形式化来看，给定一个词元序列 ${\bf u} = u_1, \ldots, u_T$，语言建模任务的目标定义为词元的预测任务：基于序列中当前位置之前的词元序列 ${\bf u}_{<t}$，采用自回归的方式对目标词元 $u_t$ 进行预测。在训练过程中，模型通常根据以下的似然函数进行优化：

$$\mathcal{L}_{LM}(u) = \sum_{t=1}^{T} \log P(u_t | {\bf u}_{<t})$$

> 具体含义如下：

> - $u$ 表示一个包含 $T$ 个单词或字符的句子，具体来说是一个序列。
> - $\mathcal{L}_{LM}(u)$ 表示语言模型对该句子的损失函数。
> - $P(u_t | {\bf u}_{<t})$ 表示在已知前 $t-1$ 个词元的条件下，第 $t$ 个词元 $u_t$ 出现的概率。
> - $\log P(u_t | {\bf u}_{<t})$ 表示这个条件概率的对数。
> - 整个公式求和表示在序列中从 $t=1$ 到 $T$ 的所有词的对数概率的总和。

---

- 此外，从本质上看，基于语言建模的预训练还可以看作是一种**任务学习过程**。例如，

    - 在预测句子前缀“这部电影演唱饱满，演员表演得也很棒，非常好看”中的“好看”时，模型实际上在进行情感分析任务的语义学习；
    - 在预测句子前缀“小明有三块糖，给了小红两块糖，还剩下一块糖”的“一块糖”时，则是在进行数学任务的语义学习。

- 可以列举出来更多类似的例子，覆盖更广的任务范围。因此，基于大规模文本语料的预训练任务能够潜在地学习到解决众多任务的相关知识与能力。

- **训练效率：Prefix Decoder < Causal Decoder**

    - Causal Decoder 结构会在**所有 token 上**计算损失，而 Prefix Decoder 只会在**输出上**计算损失。


### 去噪自编码（Denoising Autoencoding, DAE）

- 除了传统的语言建模任务外，去噪自编码任务是另一种常见的语言模型预训练任务，广泛应用于 BERT、T5 等预训练语言模型中。

- 在去噪自编码任务中，输入文本经过一系列随机替换或删除操作，形成损坏的文本 $\tilde{u}_{\backslash i}$。模型的目标是根据这些损坏的文本恢复出被替换或删除的词元片段 $\tilde{u}_i$。去噪自编码器的训练目标可以用以下数学公式表示：

$$\mathcal{L}_{DAE}(u) = \log P(\tilde{u}_i | u_{\backslash i})$$


- **与语言建模相比，去噪自编码任务的实现更为复杂**，需要设定额外的优化策略，如词元替换策略、替换片段长度、替换词元比例等。这些策略的选择会直接影响模型的训练效果。尽管去噪自编码任务在许多预训练语言模型中得到了广泛应用，然而，相比于语言建模任务，目前完全使用去噪自编码进行预训练的大语言模型还较为有限。代表性的模型包括 FLAN-T5。


### 混合去噪器（Mixture-of-Denoisers, MoD）

- 混合去噪器，通过将语言建模和去噪自编码的目标均视为不同类型的去噪任务，对预训练任务进行了统一建模。
