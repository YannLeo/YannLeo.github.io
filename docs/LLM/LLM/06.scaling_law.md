# LLM基础常见面试题（六）

## 八、什么是Scaling Law，谈谈你对它的理解

### 1. 什么是Scaling Law

#### 1.1 Scaling Law的目标

- Having a sense of the capabilities of a model **before training** can improve decisions around alignment, safety, and deployment.
  — GPT4 Technical Report
  
> 在训练之前了解模型的能力，以改善关于大模型的对齐、安全和部署的决定。

<center>
    <img src="../fig/06-GPT4_performance.png">
</center>

> 图来自OpenAI GPT-4 技术报告：https://cdn.openai.com/papers/gpt-4.pdf

>  图1. GPT-4 和较小模型的性能表现。（在训练之前，就大致预测出了GPT-4的性能边界）

> 衡量标准是OpenAI的内部代码库仿生的数据集上的最终损失。这个数据集包含了大量代码标记，并未包含在训练集中

>  我们选择观察损失，因为在不同训练计算量的情况下，损失通常比其他指标更稳定

>  虚线表示对较小模型（不包括GPT-4）的幂律拟合；该拟合可以准确预测GPT-4的最终损失。横轴为训练计算量，并进行了标准化处理（GPT-4对应的值为1）

#### 1.2 Scaling Law的定义

- **Scaling Law 定义**：
  - **用计算量、数据集规模和模型规模，来预测模型最终能力。**（通常以相对简单的函数型态，例如：线性关系、指数关系）

在大语言模型中，我们期望模型能够理解人类语言的一般规律，从而做出和人类相似的表达方式，通过使用大量的数据进行训练使模型学习到数据背后的一般规律。

在训练预训练模型时，通常有两个可以提高大语言模型性能的选项：增加数据集大小 和 增加模型中的参数量
在此基础上，训练过程中还存在一个限制条件：训练成本（如GPU的数量和可用训练时间等）。

因此，**大语言模型的预训练，通常伴随着训练的计算量、数据集规模和模型规模的三方权衡博弈**。

<center>
    <img src="../fig/06-choice.png">
</center>

但是具体，他们之间的博弈是如何展开的？

**是否可以通过 Scaling Laws 预测大模型在训练的计算量、数据集规模和模型规模这三个因素变动时，损失值的变化？**

这种预测能帮助一些关键的设计决策，比如在固定资源预算下，匹配模型的最佳大小和数据大小，而无需进行及其昂贵的试错。

### 2. OpenAI V.S DeepMind

• 基于这方面的代表性研究，主要有 OpenAI 和 DeepMind

#### 2.1 公司背景介绍

- DeepMind

> DeepMind，成立于 2010 年并于 2015 年被谷歌收购，是 Alphabet Inc. 的子公司。该公司专注于开发能模仿人类学习和解决复杂问题能力的 AI 系统。作为 Alphabet Inc. 的一部分，DeepMind 在保持高度独立的同时，也在利用谷歌的强大能力推动 AI 研究的发展。

> DeepMind 在技术上取得了显著成就，包括开发 AlphaGo，击败世界围棋冠军李世石的 AI 系统，展示了深度强化学习和神经网络的潜力，开启了一个 AI 时代。

- OpenAI

> 在谷歌收购 DeepMind 后，为避免谷歌在 AI 领域形成垄断，埃隆·马斯克和其他科技行业人物于 2015 年决定创建 OpenAI。它作为一个有声望的非营利组织，致力于开发能够推动社会进步的 AI 技术。

> 不同于 DeepMind 像一个精于解决棋盘上复杂战术的大师，专注于解决那些有明确规则和目标的难题， OpenAI 更像是一个擅长语言艺术的诗人，致力于让机器理解和生成自然的人类语言。

> 从坚持初期被外界难以理解的 GPT 路线信仰，直到拥有1750亿参数的 GPT-3 问世， OpenAI 展示了其在生成模型上无与伦比的能力，引领了另一个 AI 时代


#### 2.2 OpenAI 关于 Scaling Law 的观点

- 2020年，来自OpenAI的Kaplan等人的团队，在《Scaling Laws for Neural Language Models》论文中首次提出模拟神经语言模型的模型性能（Loss）与模型大小、数据集大小和训练计算量的关系。该团队发现三者中任何一个因素受限时，Loss与其之间存在幂律关系。

>【注：幂律指的是两个变量中的一个变量与另一个变量的某个幂次成正比。如果体现在图表中，当两个轴都是对数时，图像呈现为直线】

<center>
    <img src="../fig/06-openai.png">
</center>

> 图来自OpenAI 发表文章：Scaling Laws for Neural Language Models

> 图1 随着用于训练的计算量、数据集规模和模型规模的增加，语言建模性能平稳提升。为了获得最佳性能，必须将这三个因素同步扩大。当没有受到其他两个因素限制时，性能与每个单独因素之间呈幂律关系。

该团队的研究结论总结如下：

1. 影响模型性能的三个要素之间，每个参数会受到另外两个参数的影响。当没有其他两个瓶颈时，性能会急剧上升，影响程度为计算量 > 参数 >> 数据集大小。

2. 在固定计算预算下进行训练时，最佳性能可以通过训练参数量非常大的模型并在远离收敛前停止（Early Stopping）来实现。

3. 更大的模型在样本效率方面表现更好，能以更少的优化步骤和使用更少的数据量达到相同的性能水平。在实际应用中，应该优先考虑训练较大的模型。


#### 2.3 DeepMind 关于 Scaling Law 的观点

2022年，来自DeepMind的Hoffmann等人的团队，在《Training Compute-Optimal Large Language Models》提出了与OpenAI截然不同的观点。

**OpenAI建议在计算预算增加了10倍的情况下，如果想保持效果，模型的大小应增加5.5倍，而训练token的数量仅需增加1.8倍。**

**DeepMind这支团队则认为模型大小和训练token的数量都应该按相等的比例进行扩展**。该团队还暗示许多像GPT-3这样的千亿参数大语言模型实际上都过度参数化，也就是说它们的参数量超过了实现良好的语言理解所需，并训练不足。

该团队的研究结论总结如下：

1. 对于给定的FLOP预算，损失函数有明显的谷底值：
    
    a. 模型太小时，在较少数据上训练较大模型将是一种改进；

    b. 模型太大时，在更多数据上训练的较小模型将是一种改进。
   
    也就是说，**在给定的计算量下，数据量和模型参数之间的选择平衡存在一个最优解**。

2. 在计算成本达到最优情况下，模型大小和训练数据(token)的数量应该等比例进行缩放，即：**如果模型的大小加倍，那么训练数据的数量也应该加倍。对于给定参数量的模型，最佳的训练数据集大小约为模型中参数量的20倍**。比如，对于一个7B的模型，理想的训练数据集大小应该约为140B tokens。

3. 大模型训练需要更加关注数据集的扩展，但是**只有数据是高质量的时候，更大数据集的益处才能体现出来**。

## 3. 总结

- **Scaling Law 定义**：
    - 用计算量、数据集规模和模型规模，来预测模型最终能力。
- **OpenAI关于 Scaling Law 的主要观点**：
    - 三个要素之间，每个参数会受到另外两个参数的影响。当没有其他两个瓶颈时，性能会急剧上升，影响程度为计算量 > 参数 > 数据集大小。
- **DeepMind 关于 Scaling Law 的主要观点**：
    - 三个要素之间，应该按相等的比例进行扩展。