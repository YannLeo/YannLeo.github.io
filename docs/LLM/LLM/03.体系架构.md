# LLM体系架构

## 5. 目前主流的LLMs开源模型体系有哪些？（Prefix Decoder, Causal Decoder 和 Encoder-Decoder 区别是什么？）

- 在预训练语言模型时代，自然语言处理领域广泛采用了**预训练 + 微调**的范式，并诞生了以 **BERT 为代表的编码器（Encoder-only）架构，以 GPT 为代表的解码器（Decoder-only）架构和以 T5 为代表的编码器-解码器（Encoder-decoder）架构**的大规模预训练语言模型。

- 随着 **GPT 系列模型的成功发展**，目前自然语言处理领域走向了**生成式语言模型**的道路，解码器架构已经成为了目前大语言模型的主流架构。进一步，解码器架构还可以细分为两个变种架构，包括**因果解码器（Causal Decoder）架构**和**前缀解码器（Prefix Decoder）架构**。值得注意的是，学术界所提到解码器架构时，通常指的都是因果解码器架构。

下图针对三种架构（Causal Decoder、Prefix Decoder 和 Encoder-Decoder）进行了对比：

![](fig/03-arch.png)

### Encoder-Decoder

- Encoder-Decoder 架构是自然语言处理领域里一种经典的模型结构，广泛应用于如机器翻译等多项任务。原始的 Transformer 模型也使用了这一架构，组合了两个分别担任编码器和解码器的 Transformer 模块。

- 如下图所示，此架构**在编码器端采用了双向自注意力机制对输入信息进行编码处理，而在解码器端则使用了交叉注意力与掩码自注意力机制，进而通过自回归的方式对输出进行生成**。

- 基于编码器-解码器设计的预训练语言模型在众多自然语言理解与生成任务中展现出了优异的性能，但是目前只有如 FLAN-T5 等少数大语言模型是基于编码器-解码器架构建而成的。
